{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e0b84f-57f3-4301-a3a5-f0d96c75ceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data:\n",
      "               Correct Word               Jumbled word  Word Count\n",
      "0    நாங்கள் விளையாடினோம்.       விளையாடினோம் நாங்கள்.         2.0\n",
      "1      நான் சாப்பிடவில்லை.         சாப்பிடவில்லை நான்.         2.0\n",
      "2          இது ஒரு புத்தகம்          இது புத்தகம் ஒரு.         2.0\n",
      "3       எங்கள் வீடு பெரியது       வீடு எங்கள் பெரியது.         2.0\n",
      "4  நான் பழம் சாப்பிடுகிறேன்  பழம் நான் சாப்பிடுகிறேன்.         2.0\n",
      "Vocabulary Size: 414\n",
      "y_train shape after reshaping: (201, 5)\n",
      "y_test shape after reshaping: (51, 5)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 5, 128)            52992     \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 5, 512)            788480    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 5, 256)            787456    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5, 414)            106398    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1735326 (6.62 MB)\n",
      "Trainable params: 1735326 (6.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 5s 352ms/step - loss: 5.9967 - accuracy: 0.4617 - val_loss: 5.8739 - val_accuracy: 0.6549\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 5.7345 - accuracy: 0.6726 - val_loss: 5.0609 - val_accuracy: 0.6549\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 4.3251 - accuracy: 0.6726 - val_loss: 2.4755 - val_accuracy: 0.6549\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 2.2421 - accuracy: 0.6726 - val_loss: 2.7643 - val_accuracy: 0.6549\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 2.5847 - accuracy: 0.6726 - val_loss: 2.7614 - val_accuracy: 0.6549\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 2.3410 - accuracy: 0.6726 - val_loss: 2.4285 - val_accuracy: 0.6549\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 2.0840 - accuracy: 0.6726 - val_loss: 2.3847 - val_accuracy: 0.6549\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 2.1050 - accuracy: 0.6726 - val_loss: 2.4416 - val_accuracy: 0.6549\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 2.1215 - accuracy: 0.6726 - val_loss: 2.4356 - val_accuracy: 0.6549\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 2.0586 - accuracy: 0.6726 - val_loss: 2.4346 - val_accuracy: 0.6549\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4346 - accuracy: 0.6549\n",
      "Test Accuracy: 0.6549019813537598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aying\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = r\"D:\\H\\works\\RP\\Kaushi\\datsets\\dataset.csv\"\n",
    "df = pd.read_csv(file_path, encoding='utf-8')  # Ensuring UTF-8 encoding\n",
    "print(\"Loaded Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Clean the data (remove NaN or non-string values)\n",
    "df = df.dropna(subset=['Correct Word', 'Jumbled word'])  # Remove rows with NaN values\n",
    "df['Correct Word'] = df['Correct Word'].astype(str)  # Ensure all values are strings\n",
    "df['Jumbled word'] = df['Jumbled word'].astype(str)  # Ensure all values are strings\n",
    "\n",
    "# Step 3: Tokenize and pad the sentences\n",
    "correct_sentences = df['Correct Word'].values\n",
    "jumbled_sentences = df['Jumbled word'].values\n",
    "\n",
    "# Tokenizer to convert words into numbers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(correct_sentences + jumbled_sentences)  # Fit on both columns\n",
    "\n",
    "# Convert sentences into sequences (list of token IDs)\n",
    "correct_sequences = tokenizer.texts_to_sequences(correct_sentences)\n",
    "jumbled_sequences = tokenizer.texts_to_sequences(jumbled_sentences)\n",
    "\n",
    "# Get the vocabulary size (the total number of unique words)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "# Find the maximum sentence length (for padding)\n",
    "max_length = max([len(seq) for seq in correct_sequences])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "correct_sequences = pad_sequences(correct_sequences, maxlen=max_length, padding='post')\n",
    "jumbled_sequences = pad_sequences(jumbled_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Step 4: Split into training and testing sets (80% train, 20% test)\n",
    "X = jumbled_sequences  # Input (Jumbled sentences)\n",
    "y = correct_sequences  # Output (Correct sentences)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Reshape the labels to match the model output shape\n",
    "y_train = y_train[:, 1:]  # Remove the first token in each sentence (shift labels by 1)\n",
    "y_test = y_test[:, 1:]  # Same for test labels (shift labels by 1)\n",
    "\n",
    "# Step 6: Ensure labels have the same length as model output\n",
    "y_train = pad_sequences(y_train, maxlen=max_length, padding='post')\n",
    "y_test = pad_sequences(y_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# Check the shape of the data to ensure it's ready for training\n",
    "print(\"y_train shape after reshaping:\", y_train.shape)\n",
    "print(\"y_test shape after reshaping:\", y_test.shape)\n",
    "\n",
    "# Step 7: Build the Seq2Seq Model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding Layer: Convert words into embeddings (size of 128)\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
    "\n",
    "# Encoder: Bidirectional LSTM (captures context from both directions)\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "\n",
    "# Decoder: LSTM for sentence generation (outputs the sequence)\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "\n",
    "# Output Layer: Dense layer with softmax activation to predict the next word\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile the model with Adam optimizer and sparse categorical crossentropy loss\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary to show the architecture\n",
    "model.summary()\n",
    "\n",
    "# Step 8: Train the model with reshaped labels\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 9: Save the model after training\n",
    "model.save('sentence_reordering_model.h5')\n",
    "\n",
    "# Step 10: Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc232cb2-091d-4a74-8ff6-1bc672b8fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 911ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1656",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Predict the output\u001b[39;00m\n\u001b[0;32m      7\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(sample_pad)\n\u001b[1;32m----> 8\u001b[0m predicted_word \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_word\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted word:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_word)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1656"
     ]
    }
   ],
   "source": [
    "# Test the model with a new sentence\n",
    "sample_sentence = \"புத்தகத்தை எடு\"\n",
    "sample_seq = tokenizer.texts_to_sequences([sample_sentence])\n",
    "sample_pad = pad_sequences(sample_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Predict the output\n",
    "pred = model.predict(sample_pad)\n",
    "predicted_word = tokenizer.index_word[pred.argmax()]\n",
    "print(\"Predicted word:\", predicted_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7da39-0f2d-463b-8461-391b49a893eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
